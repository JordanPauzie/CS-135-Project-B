{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "ratings_df['movie_id_encoded'] = movie_encoder.fit_transform(ratings_df['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ratings_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure inputs are NumPy arrays\n",
    "train_user_ids = np.array(train['user_id_encoded'].values)\n",
    "train_movie_ids = np.array(train['movie_id_encoded'].values)\n",
    "train_ratings = np.array(train['rating'].values)\n",
    "\n",
    "test_user_ids = np.array(test['user_id_encoded'].values)\n",
    "test_movie_ids = np.array(test['movie_id_encoded'].values)\n",
    "test_ratings = np.array(test['rating'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define and Train the Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique users and movies\n",
    "num_users = df['user_id_encoded'].nunique()\n",
    "num_movies = df['movie_id_encoded'].nunique()\n",
    "embedding_dim = 50  # Number of dimensions for the embedding\n",
    "\n",
    "# User tower\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "# Movie tower\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
    "movie_embedding = Flatten()(movie_embedding)\n",
    "\n",
    "# Dot product of user and movie embeddings to predict rating\n",
    "dot_product = Dot(axes=1)([user_embedding, movie_embedding])\n",
    "\n",
    "# Output layer for predicting rating\n",
    "output = Dense(1, activation='linear')(dot_product)\n",
    "\n",
    "# Compile and train the model\n",
    "model2 = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model2.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history = model2.fit([train_user_ids, train_movie_ids], train_ratings, \n",
    "                    epochs=6, \n",
    "                    batch_size=64, \n",
    "                    validation_data=([test_user_ids, test_movie_ids], test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "test_loss = model2.evaluate([test_user_ids, test_movie_ids], test_ratings)\n",
    "print(f\"Test loss (MAE): {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "test_predictions = model2.predict([test_user_ids, test_movie_ids])\n",
    "\n",
    "\n",
    "# Add the original user IDs, movie IDs, actual ratings, and predicted ratings to a DataFrame\n",
    "test_results = pd.DataFrame({\n",
    "    'original_user_id': test['userId'],  # Already original IDs in `ratings_df`\n",
    "    'original_movie_id': test['movieId'],  # Match the original movie ID column\n",
    "    'actual_rating': test_ratings,\n",
    "    'predicted_rating': test_predictions.flatten()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the prediction ranking difference in order(for a specific user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference between actual and predicted ratings\n",
    "test_results['rating_difference'] = abs(test_results['actual_rating'] - test_results['predicted_rating'])\n",
    "\n",
    "# Filter by a specific user ID (adjust for your data type)\n",
    "filter_user_id = 772  # Replace with the desired user ID\n",
    "filtered_results = test_results[test_results['original_user_id'] == filter_user_id]\n",
    "\n",
    "# Sort by the closest rating difference\n",
    "filtered_results_sorted = filtered_results.sort_values(by='rating_difference')\n",
    "\n",
    "# Display sorted results\n",
    "print(f\"Predictions for user {filter_user_id}, ordered by closest rating difference:\")\n",
    "print(filtered_results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user_ids and item_ids from the leaderboard data\n",
    "user_ids = np.array(leaderboard_data['user_id'].values)\n",
    "item_ids = np.array(leaderboard_data['item_id'].values)\n",
    "\n",
    "predictions = model2.predict([user_ids, item_ids])\n",
    "\n",
    "# Save the predictions to a text file\n",
    "with open(\"predicted_ratings_leaderboard3.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_ids.max(), item_ids.max())\n",
    "print(user_ids.min(), item_ids.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_user_ids.max(), test_movie_ids.max())\n",
    "print(test_user_ids.min(), test_movie_ids.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert 'userId' in ratings_df.columns and 'movieId' in ratings_df.columns and 'rating' in ratings_df.columns\n",
    "\n",
    "# Encode user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "ratings_df['movie_id_encoded'] = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "\n",
    "# Copy the dataset and split into train and test sets\n",
    "df = ratings_df.copy()\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert columns to NumPy arrays\n",
    "train_user_ids = np.array(train['user_id_encoded'].values)\n",
    "train_movie_ids = np.array(train['movie_id_encoded'].values)\n",
    "train_ratings = np.array(train['rating'].values)\n",
    "\n",
    "test_user_ids = np.array(test['user_id_encoded'].values)\n",
    "test_movie_ids = np.array(test['movie_id_encoded'].values)\n",
    "test_ratings = np.array(test['rating'].values)\n",
    "\n",
    "# Define the number of unique users, movies, and embedding dimensions\n",
    "num_users = df['user_id_encoded'].nunique()\n",
    "num_movies = df['movie_id_encoded'].nunique()\n",
    "embedding_dim = 50\n",
    "\n",
    "# Define the model\n",
    "# User input and embedding\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "# Movie input and embedding\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
    "movie_embedding = Flatten()(movie_embedding)\n",
    "\n",
    "# Dot product of embeddings and output layer\n",
    "dot_product = Dot(axes=1)([user_embedding, movie_embedding])\n",
    "output = Dense(1, activation='linear')(dot_product)\n",
    "\n",
    "# Compile the model\n",
    "model2 = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model2.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(\n",
    "    [train_user_ids, train_movie_ids], train_ratings,\n",
    "    epochs=6,\n",
    "    batch_size=64,\n",
    "    validation_data=([test_user_ids, test_movie_ids], test_ratings),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model2.evaluate([test_user_ids, test_movie_ids], test_ratings)\n",
    "print(f\"Test loss (MAE): {test_loss}\")\n",
    "\n",
    "# Predict ratings for the test set\n",
    "test_predictions = model2.predict([test_user_ids, test_movie_ids])\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'original_user_id': test['userId'],\n",
    "    'original_movie_id': test['movieId'],\n",
    "    'actual_rating': test_ratings,\n",
    "    'predicted_rating': test_predictions.flatten()\n",
    "})\n",
    "test_results['rating_difference'] = abs(test_results['actual_rating'] - test_results['predicted_rating'])\n",
    "\n",
    "# Filter results for a specific user and sort by rating difference\n",
    "filter_user_id = 772  # Replace with your desired user ID\n",
    "filtered_results = test_results[test_results['original_user_id'] == filter_user_id]\n",
    "filtered_results_sorted = filtered_results.sort_values(by='rating_difference')\n",
    "\n",
    "print(f\"Predictions for user {filter_user_id}, ordered by closest rating difference:\")\n",
    "print(filtered_results_sorted)\n",
    "\n",
    "# Load leaderboard data\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv')  # Replace with your leaderboard file path\n",
    "assert 'user_id' in leaderboard_data.columns and 'item_id' in leaderboard_data.columns\n",
    "\n",
    "# Handle unseen user and movie IDs using mapping\n",
    "user_mapping = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "movie_mapping = dict(zip(movie_encoder.classes_, movie_encoder.transform(movie_encoder.classes_)))\n",
    "\n",
    "leaderboard_data['user_id_encoded'] = leaderboard_data['user_id'].map(user_mapping).fillna(-1).astype(int)\n",
    "leaderboard_data['item_id_encoded'] = leaderboard_data['item_id'].map(movie_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Predict leaderboard ratings, handling valid IDs only\n",
    "user_ids = leaderboard_data['user_id_encoded'].values\n",
    "item_ids = leaderboard_data['item_id_encoded'].values\n",
    "\n",
    "valid_indices = (user_ids != -1) & (item_ids != -1)\n",
    "predictions = np.full(len(user_ids), np.nan)  # Initialize with NaN\n",
    "predictions[valid_indices] = model2.predict([user_ids[valid_indices], item_ids[valid_indices]]).flatten()\n",
    "\n",
    "# Save leaderboard predictions to a file\n",
    "with open(\"predicted_ratings_leaderboard3.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        if np.isnan(pred):  # Handle invalid predictions\n",
    "            f.write(\"Invalid\\n\")\n",
    "        else:\n",
    "            f.write(f\"{pred:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_valid_test_loader import load_train_valid_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/movie_info.csv')\n",
    "ratings_df = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "# print(movies_df)\n",
    "# Rename columns\n",
    "ratings_df = ratings_df.rename(columns={'user_id': 'userId', 'item_id': 'movieId'})\n",
    "movies_df = movies_df.rename(columns={'item_id': 'movieId'})\n",
    "\n",
    "\n",
    "# Add a new column 'timestamp' filled with zeros\n",
    "ratings_df['timestamp'] = 0\n",
    "\n",
    "\n",
    "# print(ratings_df)\n",
    "print(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmO7jmrpkvZf"
   },
   "outputs": [],
   "source": [
    "# # import the dataset\n",
    "# import pandas as pd\n",
    "# movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
    "# ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-KRTGaskvdn",
    "outputId": "ec89b4c8-ee99-407c-ce07-8d9ddcf0f0b1"
   },
   "outputs": [],
   "source": [
    "print('The dimensions of movies dataframe are:', movies_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "ALPeuAiBk0mM",
    "outputId": "8afc063a-7b73-4cdf-b624-cf36ecfa231a"
   },
   "outputs": [],
   "source": [
    "# Take a look at movies_df\n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "KzPoqkWbk0oh",
    "outputId": "762a17ed-9513-48f8-f1ef-0c1b63347624"
   },
   "outputs": [],
   "source": [
    "# Take a look at ratings_df\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2TUaWt96k0q7",
    "outputId": "afd13baa-d03e-40c9-9dcf-f8c4cbda08d0"
   },
   "outputs": [],
   "source": [
    "# Movie ID to movie name mapping\n",
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "print(\"Movie names:\", movie_names)\n",
    "print(\"Number of unique users:\", n_users)\n",
    "print(\"Number of unique movies:\", n_items)\n",
    "print(\"The full rating matrix will have:\", n_users*n_items, 'elements.')\n",
    "print('----------')\n",
    "print(\"Number of ratings:\", len(ratings_df))\n",
    "print(\"Therefore: \", len(ratings_df) / (n_users*n_items) * 100, '% of the matrix is filled.')\n",
    "print(\"We have an incredibly sparse matrix to work with here.\")\n",
    "print(\"And... as you can imagine, as the number of users and products grow, the number of elements will increase by n*2\")\n",
    "print(\"You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\")\n",
    "print(\"One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YnkTvMsCk0tY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "class MatrixFactorization(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors) # think of this as a lookup table for the input.\n",
    "        # create item embeddings\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors) # think of this as a lookup table for the input.\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.item_factors.weight.data.uniform_(0, 0.05)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication\n",
    "        users, items = data[:,0], data[:,1]\n",
    "        return (self.user_factors(users)*self.item_factors(items)).sum(1)\n",
    "    # def forward(self, user, item):\n",
    "    # \t# matrix multiplication\n",
    "    #     return (self.user_factors(user)*self.item_factors(item)).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return self.forward(user, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LTlGvz7sk0v2"
   },
   "outputs": [],
   "source": [
    "# Creating the dataloader (necessary for PyTorch)\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader # package that helps transform your data to machine learning readiness\n",
    "\n",
    "# Note: This isn't 'good' practice, in a MLops sense but we'll roll with this since the data is already loaded in memory.\n",
    "class Loader(Dataset):\n",
    "    def __init__(self):\n",
    "        self.ratings = ratings_df.copy()\n",
    "        \n",
    "        # Extract all user IDs and movie IDs\n",
    "        users = ratings_df.userId.unique()\n",
    "        movies = ratings_df.movieId.unique()\n",
    "        \n",
    "        #--- Producing new continuous IDs for users and movies ---\n",
    "        \n",
    "        # Unique values : index\n",
    "        self.userid2idx = {o:i for i,o in enumerate(users)}\n",
    "        self.movieid2idx = {o:i for i,o in enumerate(movies)}\n",
    "        \n",
    "        # Obtained continuous ID for users and movies\n",
    "        self.idx2userid = {i:o for o,i in self.userid2idx.items()}\n",
    "        self.idx2movieid = {i:o for o,i in self.movieid2idx.items()}\n",
    "        \n",
    "        # return the id from the indexed values as noted in the lambda function down below.\n",
    "        self.ratings.movieId = ratings_df.movieId.apply(lambda x: self.movieid2idx[x])\n",
    "        self.ratings.userId = ratings_df.userId.apply(lambda x: self.userid2idx[x])\n",
    "        \n",
    "        \n",
    "        self.x = self.ratings.drop(['rating', 'timestamp'], axis=1).values\n",
    "        self.y = self.ratings['rating'].values\n",
    "        self.x, self.y = torch.tensor(self.x), torch.tensor(self.y) # Transforms the data to tensors (ready for torch models.)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6MK3Ymmk0yi",
    "outputId": "50e960e6-ccc7-4852-98cf-55144f7848c7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = MatrixFactorization(n_users, n_items, n_factors=8)\n",
    "print(model)\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "# GPU enable if you have a GPU...\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss\n",
    "loss_fn = torch.nn.L1Loss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = Loader()\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zs_GVbSYmhib",
    "outputId": "d4a5a7f6-f2a6-4d08-9083-84a2812c3f45"
   },
   "outputs": [],
   "source": [
    "# By training the model, we will have tuned latent factors for movies and users.\n",
    "c = 0\n",
    "uw = 0\n",
    "iw = 0 \n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "        if c == 0:\n",
    "          uw = param.data\n",
    "          c +=1\n",
    "        else:\n",
    "          iw = param.data\n",
    "        #print('param_data', param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IJgie01_p8k5"
   },
   "outputs": [],
   "source": [
    "trained_movie_embeddings = model.item_factors.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-_tHZ_E_rub",
    "outputId": "cb97ce5d-7140-43fa-8ffa-7603a22eee2c"
   },
   "outputs": [],
   "source": [
    "len(trained_movie_embeddings) # unique movie factor weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bl9s4iqXy75q"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8MDzp-9u-m5n",
    "outputId": "c6f77e4d-68d1-49c5-c022-25e80e99007d"
   },
   "outputs": [],
   "source": [
    "'''It can be seen here that the movies that are in the same cluster tend to have\n",
    "similar genres. Also note that the algorithm is unfamiliar with the movie name\n",
    "and only obtained the relationships by looking at the numbers representing how\n",
    "users have responded to the movie selections.'''\n",
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  movs = []\n",
    "  for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    movid = train_set.idx2movieid[movidx]\n",
    "     # Check how many ratings this movie has rat_count = ratings_df.loc[ratings_df['movieId'] == movid].count().iloc[0]\n",
    "    rat_count = ratings_df.loc[ratings_df['movieId'] == movid].count().iloc[0]\n",
    "    movs.append((movie_names[movid], rat_count))\n",
    "  for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", mov[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each movie to a cluster\n",
    "movie_clusters = {}\n",
    "for cluster in range(10):\n",
    "    for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "        movid = train_set.idx2movieid[movidx]\n",
    "        movie_clusters[movid] = cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Enhance the Training Data: Add the cluster information to the ratings data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cluster information to ratings_df\n",
    "ratings_df['cluster'] = ratings_df['movieId'].map(movie_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Train a Model for Each Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Placeholder for cluster models\n",
    "cluster_models = {}\n",
    "\n",
    "# Train a model for each cluster\n",
    "for cluster in range(10):\n",
    "    cluster_data = ratings_df[ratings_df['cluster'] == cluster]\n",
    "    \n",
    "    if cluster_data.empty:\n",
    "        continue  # Skip empty clusters\n",
    "\n",
    "    # Prepare data for the surprise library\n",
    "    reader = Reader(rating_scale=(0.5, 5))\n",
    "    data = Dataset.load_from_df(cluster_data[['userId', 'movieId', 'rating']], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "    \n",
    "    # Train an SVD model\n",
    "    model = SVD()\n",
    "    model.fit(trainset)\n",
    "    cluster_models[cluster] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Predict Ratings for Leaderboard Data: For each movie-user pair in the leaderboard data, use the appropriate cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leaderboard data\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "leaderboard_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leaderboard data\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv')\n",
    "\n",
    "# Predict ratings\n",
    "predicted_ratings = []\n",
    "\n",
    "for row in leaderboard_data.itertuples(index=False):\n",
    "    user_id = row.user_id  # Assuming the column is named 'userId' in the CSV\n",
    "    movie_id = row.item_id  # Assuming the column is named 'movieId' in the CSV\n",
    "    \n",
    "    cluster = movie_clusters.get(movie_id, None)\n",
    "    \n",
    "    if cluster is not None and cluster in cluster_models:\n",
    "        # Use the model for the cluster\n",
    "        pred = cluster_models[cluster].predict(user_id, movie_id).est\n",
    "    else:\n",
    "        # Fallback to a global average if no cluster or model exists\n",
    "        pred = ratings_df['rating'].mean()\n",
    "    \n",
    "    predicted_ratings.append(pred)\n",
    "\n",
    "# Save predictions to a file\n",
    "np.savetxt('predicted_ratings_leaderboard.txt', predicted_ratings, fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CollabFilterOneVectorPerItem import *\n",
    "from AbstractBaseCollabFilterSGD import *\n",
    "from train_valid_test_loader import load_train_valid_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()\n",
    "model = CollabFilterOneVectorPerItem(n_epochs=10, batch_size=50, step_size=0.1, n_factors = 50, alpha=0.001)\n",
    "model.init_parameter_dict(n_users, n_items, train_tuple)\n",
    "model.fit(train_tuple, valid_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CollabFilterOneVectorPerItem import *\n",
    "from AbstractBaseCollabFilterSGD import *\n",
    "from train_valid_test_loader import load_train_valid_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()\n",
    "model = CollabFilterOneVectorPerItem(n_epochs=10, batch_size=50, step_size=0.1, n_factors = 50, alpha=0.001)\n",
    "model.init_parameter_dict(n_users, n_items, train_tuple)\n",
    "model.fit(train_tuple, valid_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user_ids and item_ids from the leaderboard data\n",
    "user_ids = leaderboard_data['user_id'].to_numpy()\n",
    "item_ids = leaderboard_data['item_id'].to_numpy()\n",
    "\n",
    "# Call the provided prediction function using these user_ids and item_ids\n",
    "# Assuming the model parameters (mu, b_per_user, c_per_item, U, V) are defined or loaded\n",
    "predictions = model.predict(user_ids, item_ids)\n",
    "\n",
    "# Save the predictions to a text file\n",
    "with open(\"predicted_ratings_leaderboard.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CollabFilterOneVectorPerItem import *\n",
    "from AbstractBaseCollabFilterSGD import *\n",
    "from train_valid_test_loader import load_train_valid_test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tuple, valid_tuple, test_tuple, n_users, n_items = load_train_valid_test_datasets()\n",
    "model = CollabFilterOneVectorPerItem(n_epochs=10, batch_size=50, step_size=0.1, n_factors = 50, alpha=0.001)\n",
    "model.init_parameter_dict(n_users, n_items, train_tuple)\n",
    "model.fit(train_tuple, valid_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract user_ids and item_ids from the leaderboard data\n",
    "user_ids = leaderboard_data['user_id'].to_numpy()\n",
    "item_ids = leaderboard_data['item_id'].to_numpy()\n",
    "\n",
    "# Call the provided prediction function using these user_ids and item_ids\n",
    "# Assuming the model parameters (mu, b_per_user, c_per_item, U, V) are defined or loaded\n",
    "predictions = model.predict(user_ids, item_ids)\n",
    "\n",
    "# Save the predictions to a text file\n",
    "with open(\"predicted_ratings_leaderboard2.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Tower Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 2.9844 - val_loss: 0.8454\n",
      "Epoch 2/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7902 - val_loss: 0.7691\n",
      "Epoch 3/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7209 - val_loss: 0.7538\n",
      "Epoch 4/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6834 - val_loss: 0.7448\n",
      "Epoch 5/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6479 - val_loss: 0.7432\n",
      "Epoch 6/6\n",
      "\u001b[1m1125/1125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.6098 - val_loss: 0.7412\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 566us/step - loss: 0.7421\n",
      "Test loss (MAE): 0.7412182688713074\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 590us/step\n",
      "Predictions for user 772, ordered by closest rating difference:\n",
      "       original_user_id  original_movie_id  actual_rating  predicted_rating  \\\n",
      "525                 772                175              4          3.887517   \n",
      "10398               772                946              2          2.152109   \n",
      "74901               772                187              3          3.257831   \n",
      "21632               772                284              3          3.332614   \n",
      "44904               772                 69              3          3.352849   \n",
      "67026               772                893              2          2.365895   \n",
      "62191               772                917              5          4.626688   \n",
      "76753               772                230              2          2.531592   \n",
      "8138                772                431              3          3.617760   \n",
      "13524               772                565              2          2.657907   \n",
      "89249               772                391              2          2.712318   \n",
      "74708               772                126              5          4.240340   \n",
      "26294               772                401              2          2.821555   \n",
      "19940               772               1019              5          4.030609   \n",
      "89978               772                728              3          3.993068   \n",
      "56211               772                170              5          3.960131   \n",
      "71188               772                183              2          3.062603   \n",
      "38949               772                 95              2          3.073854   \n",
      "70790               772               1034              3          1.822649   \n",
      "57334               772                  6              2          3.801926   \n",
      "63078               772                180              5          3.127131   \n",
      "89032               772                922              1          3.345617   \n",
      "78013               772                257              5          2.610977   \n",
      "\n",
      "       rating_difference  \n",
      "525             0.112483  \n",
      "10398           0.152109  \n",
      "74901           0.257831  \n",
      "21632           0.332614  \n",
      "44904           0.352849  \n",
      "67026           0.365895  \n",
      "62191           0.373312  \n",
      "76753           0.531592  \n",
      "8138            0.617760  \n",
      "13524           0.657907  \n",
      "89249           0.712318  \n",
      "74708           0.759660  \n",
      "26294           0.821555  \n",
      "19940           0.969391  \n",
      "89978           0.993068  \n",
      "56211           1.039869  \n",
      "71188           1.062603  \n",
      "38949           1.073854  \n",
      "70790           1.177351  \n",
      "57334           1.801926  \n",
      "63078           1.872869  \n",
      "89032           2.345617  \n",
      "78013           2.389023  \n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486us/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Ensure required columns exist\n",
    "assert 'userId' in ratings_df.columns and 'movieId' in ratings_df.columns and 'rating' in ratings_df.columns\n",
    "\n",
    "# Encode user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "ratings_df['movie_id_encoded'] = movie_encoder.fit_transform(ratings_df['movieId'])\n",
    "\n",
    "# Copy the dataset and split into train and test sets\n",
    "df = ratings_df.copy()\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert columns to NumPy arrays\n",
    "train_user_ids = np.array(train['user_id_encoded'].values)\n",
    "train_movie_ids = np.array(train['movie_id_encoded'].values)\n",
    "train_ratings = np.array(train['rating'].values)\n",
    "\n",
    "test_user_ids = np.array(test['user_id_encoded'].values)\n",
    "test_movie_ids = np.array(test['movie_id_encoded'].values)\n",
    "test_ratings = np.array(test['rating'].values)\n",
    "\n",
    "# Define the number of unique users, movies, and embedding dimensions\n",
    "num_users = df['user_id_encoded'].nunique()\n",
    "num_movies = df['movie_id_encoded'].nunique()\n",
    "embedding_dim = 50\n",
    "\n",
    "# Define the model\n",
    "# User input and embedding\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "# Movie input and embedding\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
    "movie_embedding = Flatten()(movie_embedding)\n",
    "\n",
    "# Dot product of embeddings and output layer\n",
    "dot_product = Dot(axes=1)([user_embedding, movie_embedding])\n",
    "output = Dense(1, activation='linear')(dot_product)\n",
    "\n",
    "# Compile the model\n",
    "model2 = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model2.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model2.fit(\n",
    "    [train_user_ids, train_movie_ids], train_ratings,\n",
    "    epochs=6,\n",
    "    batch_size=64,\n",
    "    validation_data=([test_user_ids, test_movie_ids], test_ratings),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss = model2.evaluate([test_user_ids, test_movie_ids], test_ratings)\n",
    "print(f\"Test loss (MAE): {test_loss}\")\n",
    "\n",
    "# Predict ratings for the test set\n",
    "test_predictions = model2.predict([test_user_ids, test_movie_ids])\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'original_user_id': test['userId'],\n",
    "    'original_movie_id': test['movieId'],\n",
    "    'actual_rating': test_ratings,\n",
    "    'predicted_rating': test_predictions.flatten()\n",
    "})\n",
    "test_results['rating_difference'] = abs(test_results['actual_rating'] - test_results['predicted_rating'])\n",
    "\n",
    "# Filter results for a specific user and sort by rating difference\n",
    "filter_user_id = 772  # Replace with your desired user ID\n",
    "filtered_results = test_results[test_results['original_user_id'] == filter_user_id]\n",
    "filtered_results_sorted = filtered_results.sort_values(by='rating_difference')\n",
    "\n",
    "print(f\"Predictions for user {filter_user_id}, ordered by closest rating difference:\")\n",
    "print(filtered_results_sorted)\n",
    "\n",
    "# Load leaderboard data\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv')  # Replace with your leaderboard file path\n",
    "assert 'user_id' in leaderboard_data.columns and 'item_id' in leaderboard_data.columns\n",
    "\n",
    "# Handle unseen user and movie IDs using mapping\n",
    "user_mapping = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "movie_mapping = dict(zip(movie_encoder.classes_, movie_encoder.transform(movie_encoder.classes_)))\n",
    "\n",
    "leaderboard_data['user_id_encoded'] = leaderboard_data['user_id'].map(user_mapping).fillna(-1).astype(int)\n",
    "leaderboard_data['item_id_encoded'] = leaderboard_data['item_id'].map(movie_mapping).fillna(-1).astype(int)\n",
    "\n",
    "# Predict leaderboard ratings, handling valid IDs only\n",
    "user_ids = leaderboard_data['user_id_encoded'].values\n",
    "item_ids = leaderboard_data['item_id_encoded'].values\n",
    "\n",
    "valid_indices = (user_ids != -1) & (item_ids != -1)\n",
    "predictions = np.full(len(user_ids), np.nan)  # Initialize with NaN\n",
    "predictions[valid_indices] = model2.predict([user_ids[valid_indices], item_ids[valid_indices]]).flatten()\n",
    "\n",
    "# Save leaderboard predictions to a file\n",
    "with open(\"predicted_ratings_leaderboard3.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        if np.isnan(pred):  # Handle invalid predictions\n",
    "            f.write(\"Invalid\\n\")\n",
    "        else:\n",
    "            f.write(f\"{pred}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Recommendation_System.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "005a499c40bd407ca20c2492d9c4f170": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b4738815cd467dba2946031d6d9027": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c1765cabeaf4d75a130311ddc4591a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_69073fd458234bba829af9731b4fce1c",
       "IPY_MODEL_88151edb53134487b95f6bcbcd7fd2b4"
      ],
      "layout": "IPY_MODEL_6db170b52f5a4f72afd083a3a9c157aa"
     }
    },
    "69073fd458234bba829af9731b4fce1c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20b4738815cd467dba2946031d6d9027",
      "max": 128,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9b185906b1dc465e93b9f058a1fec47a",
      "value": 128
     }
    },
    "6db170b52f5a4f72afd083a3a9c157aa": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88151edb53134487b95f6bcbcd7fd2b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_005a499c40bd407ca20c2492d9c4f170",
      "placeholder": "​",
      "style": "IPY_MODEL_c279f9a0a0fe498b9db167a237517e6e",
      "value": " 128/128 [03:03&lt;00:00,  1.44s/it]"
     }
    },
    "9b185906b1dc465e93b9f058a1fec47a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c279f9a0a0fe498b9db167a237517e6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
