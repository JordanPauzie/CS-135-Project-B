{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/movie_info.csv')\n",
    "ratings_df = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_all_development_set.csv')\n",
    "ratings_df = ratings_df.rename(columns={'user_id': 'userId', 'item_id': 'movieId'})\n",
    "movies_df = movies_df.rename(columns={'item_id': 'movieId'})\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure required columns exist\n",
    "assert 'userId' in ratings_df.columns and 'movieId' in ratings_df.columns and 'rating' in ratings_df.columns\n",
    "\n",
    "# Encode user and movie IDs\n",
    "user_encoder = LabelEncoder()\n",
    "ratings_df['user_id_encoded'] = user_encoder.fit_transform(ratings_df['userId'])\n",
    "\n",
    "movie_encoder = LabelEncoder()\n",
    "ratings_df['movie_id_encoded'] = movie_encoder.fit_transform(ratings_df['movieId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset and split into train and test sets\n",
    "df = ratings_df.copy()\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert columns to NumPy arrays\n",
    "train_user_ids = np.array(train['user_id_encoded'].values)\n",
    "train_movie_ids = np.array(train['movie_id_encoded'].values)\n",
    "train_ratings = np.array(train['rating'].values)\n",
    "\n",
    "test_user_ids = np.array(test['user_id_encoded'].values)\n",
    "test_movie_ids = np.array(test['movie_id_encoded'].values)\n",
    "test_ratings = np.array(test['rating'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of unique users, movies, and embedding dimensions\n",
    "num_users = df['user_id_encoded'].nunique()\n",
    "num_movies = df['movie_id_encoded'].nunique()\n",
    "embedding_dim = 128\n",
    "\n",
    "# Define the model\n",
    "# User input and embedding\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "# Movie input and embedding\n",
    "movie_input = Input(shape=(1,), name='movie_input')\n",
    "movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
    "movie_embedding = Flatten()(movie_embedding)\n",
    "\n",
    "# Dot product of embeddings and output layer\n",
    "dot_product = Dot(axes=1)([user_embedding, movie_embedding])\n",
    "output = Dense(1, activation='linear')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4280 - val_loss: 1.9832\n",
      "Epoch 2/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2575 - val_loss: 0.8015\n",
      "Epoch 3/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7564 - val_loss: 0.7628\n",
      "Epoch 4/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7071 - val_loss: 0.7515\n",
      "Epoch 5/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6823 - val_loss: 0.7401\n",
      "Epoch 6/6\n",
      "\u001b[1m282/282\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.6487 - val_loss: 0.7362\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Add early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    [train_user_ids, train_movie_ids], train_ratings,\n",
    "    epochs=6,\n",
    "    batch_size=256,\n",
    "    validation_data=([test_user_ids, test_movie_ids], test_ratings),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 454us/step - loss: 0.7383\n",
      "Test loss (MAE): 0.7361571192741394\n",
      "\u001b[1m563/563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 507us/step\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss = model.evaluate([test_user_ids, test_movie_ids], test_ratings)\n",
    "print(f\"Test loss (MAE): {test_loss}\")\n",
    "\n",
    "# Predict ratings for the test set\n",
    "test_predictions = model.predict([test_user_ids, test_movie_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'original_user_id': test['userId'],\n",
    "    'original_movie_id': test['movieId'],\n",
    "    'actual_rating': test_ratings,\n",
    "    'predicted_rating': test_predictions.flatten()\n",
    "})\n",
    "test_results['rating_difference'] = abs(test_results['actual_rating'] - test_results['predicted_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for user 772, ordered by closest rating difference:\n",
      "       original_user_id  original_movie_id  actual_rating  predicted_rating  \\\n",
      "44904               772                 69              3          3.323037   \n",
      "21632               772                284              3          3.335831   \n",
      "10398               772                946              2          1.663316   \n",
      "525                 772                175              4          3.655008   \n",
      "76753               772                230              2          2.493973   \n",
      "8138                772                431              3          3.526340   \n",
      "74901               772                187              3          3.526487   \n",
      "67026               772                893              2          2.668275   \n",
      "74708               772                126              5          4.283081   \n",
      "13524               772                565              2          2.717553   \n",
      "89249               772                391              2          2.737000   \n",
      "89978               772                728              3          3.931743   \n",
      "26294               772                401              2          2.942775   \n",
      "70790               772               1034              3          1.980163   \n",
      "71188               772                183              2          3.133855   \n",
      "56211               772                170              5          3.819510   \n",
      "62191               772                917              5          3.817226   \n",
      "38949               772                 95              2          3.303536   \n",
      "19940               772               1019              5          3.624564   \n",
      "57334               772                  6              2          3.445616   \n",
      "63078               772                180              5          3.220839   \n",
      "78013               772                257              5          3.051689   \n",
      "89032               772                922              1          3.297488   \n",
      "\n",
      "       rating_difference  \n",
      "44904           0.323037  \n",
      "21632           0.335831  \n",
      "10398           0.336684  \n",
      "525             0.344992  \n",
      "76753           0.493973  \n",
      "8138            0.526340  \n",
      "74901           0.526487  \n",
      "67026           0.668275  \n",
      "74708           0.716919  \n",
      "13524           0.717553  \n",
      "89249           0.737000  \n",
      "89978           0.931743  \n",
      "26294           0.942775  \n",
      "70790           1.019837  \n",
      "71188           1.133855  \n",
      "56211           1.180490  \n",
      "62191           1.182774  \n",
      "38949           1.303536  \n",
      "19940           1.375436  \n",
      "57334           1.445616  \n",
      "63078           1.779161  \n",
      "78013           1.948311  \n",
      "89032           2.297488  \n"
     ]
    }
   ],
   "source": [
    "# Filter results for a specific user and sort by rating difference\n",
    "filter_user_id = 772  # Replace with your desired user ID\n",
    "filtered_results = test_results[test_results['original_user_id'] == filter_user_id]\n",
    "filtered_results_sorted = filtered_results.sort_values(by='rating_difference')\n",
    "\n",
    "print(f\"Predictions for user {filter_user_id}, ordered by closest rating difference:\")\n",
    "print(filtered_results_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load leaderboard data\n",
    "leaderboard_data = pd.read_csv('/Users/brandonmukadziwashe/CS135/cs135-24f-assignments/CS-135-Project-B/data_movie_lens_100k/ratings_masked_leaderboard_set.csv')  # Replace with your leaderboard file path\n",
    "assert 'user_id' in leaderboard_data.columns and 'item_id' in leaderboard_data.columns\n",
    "\n",
    "# Handle unseen user and movie IDs using mapping\n",
    "user_mapping = dict(zip(user_encoder.classes_, user_encoder.transform(user_encoder.classes_)))\n",
    "movie_mapping = dict(zip(movie_encoder.classes_, movie_encoder.transform(movie_encoder.classes_)))\n",
    "\n",
    "leaderboard_data['user_id_encoded'] = leaderboard_data['user_id'].map(user_mapping).fillna(-1).astype(int)\n",
    "leaderboard_data['item_id_encoded'] = leaderboard_data['item_id'].map(movie_mapping).fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'ModuleNotFoundError' has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m (user_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m (item_ids \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(\u001b[38;5;28mlen\u001b[39m(user_ids), np\u001b[38;5;241m.\u001b[39mnan)  \u001b[38;5;66;03m# Initialize with NaN\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m predictions[valid_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;43;01mModuleNotFoundError\u001b[39;49;00m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m([user_ids[valid_indices], item_ids[valid_indices]])\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Save leaderboard predictions to a file\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_ratings_leaderboard4.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'ModuleNotFoundError' has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "# Predict leaderboard ratings, handling valid IDs only\n",
    "user_ids = leaderboard_data['user_id_encoded'].values\n",
    "item_ids = leaderboard_data['item_id_encoded'].values\n",
    "\n",
    "valid_indices = (user_ids != -1) & (item_ids != -1)\n",
    "predictions = np.full(len(user_ids), np.nan)  # Initialize with NaN\n",
    "predictions[valid_indices] = ModuleNotFoundError.predict([user_ids[valid_indices], item_ids[valid_indices]]).flatten()\n",
    "\n",
    "# Save leaderboard predictions to a file\n",
    "with open(\"predicted_ratings_leaderboard4.txt\", \"w\") as f:\n",
    "    for pred in predictions:\n",
    "        if np.isnan(pred):  # Handle invalid predictions\n",
    "            f.write(\"Invalid\\n\")\n",
    "        else:\n",
    "            f.write(f\"{pred}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 Report Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Proposed Method Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our proposed method is a neural collaborative filtering approach that employs user and movie embeddings to predict ratings. The model is designed with embedding layers for users and movies, which are multiplied to capture latent interactions. A dense layer further refines these interactions, and the output is a single predicted rating.\n",
    "\n",
    "We chose this method because neural collaborative filtering has demonstrated success in capturing complex user-item relationships in sparse datasets. Hyperparameters such as embedding dimensions, batch size, and learning rate were tuned using systematic experimentation. Early stopping was employed to prevent overfitting, ensuring robust performance across validation and test sets. This design balances simplicity and predictive power, making it suitable for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# def build_and_train_model(num_users, num_movies, embedding_dim, learning_rate, batch_size, epochs, patience):\n",
    "#     # Define the model\n",
    "#     user_input = Input(shape=(1,), name='user_input')\n",
    "#     user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim, name='user_embedding')(user_input)\n",
    "#     user_embedding = Flatten()(user_embedding)\n",
    "\n",
    "#     movie_input = Input(shape=(1,), name='movie_input')\n",
    "#     movie_embedding = Embedding(input_dim=num_movies, output_dim=embedding_dim, name='movie_embedding')(movie_input)\n",
    "#     movie_embedding = Flatten()(movie_embedding)\n",
    "\n",
    "#     dot_product = Dot(axes=1)([user_embedding, movie_embedding])\n",
    "#     output = Dense(1, activation='linear')(dot_product)\n",
    "\n",
    "#     model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "#     model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "#     # Add early stopping\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
    "\n",
    "#     # Train the model\n",
    "#     history = model.fit(\n",
    "#         [train_user_ids, train_movie_ids], train_ratings,\n",
    "#         epochs=epochs,\n",
    "#         batch_size=batch_size,\n",
    "#         validation_data=([test_user_ids, test_movie_ids], test_ratings),\n",
    "#         callbacks=[early_stopping],\n",
    "#         verbose=0\n",
    "#     )\n",
    "\n",
    "#     # Evaluate the model\n",
    "#     val_loss = model.evaluate([test_user_ids, test_movie_ids], test_ratings, verbose=0)\n",
    "#     return model, val_loss\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# embedding_dims = [16, 32, 64, 128]\n",
    "# learning_rates = [0.001, 0.005, 0.01, 0.05]\n",
    "# batch_sizes = [32, 64, 128, 256]\n",
    "# epochs = 10\n",
    "# patience = 3\n",
    "\n",
    "# best_model = None\n",
    "# best_params = None\n",
    "# lowest_mae = float('inf')\n",
    "\n",
    "# # Perform grid search\n",
    "# for emb_dim, lr, batch_size in itertools.product(embedding_dims, learning_rates, batch_sizes):\n",
    "#     print(f\"Trying: Embedding Dim={emb_dim}, Learning Rate={lr}, Batch Size={batch_size}\")\n",
    "#     model, val_mae = build_and_train_model(num_users, num_movies, emb_dim, 0.001, batch_size, epochs, patience)\n",
    "    \n",
    "#     print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "    \n",
    "#     if val_mae < lowest_mae:\n",
    "#         lowest_mae = val_mae\n",
    "#         best_model = model\n",
    "#         best_params = (emb_dim, lr, batch_size)\n",
    "\n",
    "# print(f\"\\nBest Model Found: Embedding Dim={best_params[0]}, Learning Rate={best_params[1]}, Batch Size={best_params[2]}\")\n",
    "# print(f\"Lowest Validation MAE: {lowest_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
